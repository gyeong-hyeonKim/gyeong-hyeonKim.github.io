---
layout: single
title: "2025 대한조선학회 춘계학술대회"
categories: conference
tag: [RL,]
date: 2025-05-09
typora-copy-images-to: ../images/2025springconference
---

### **프로젝트 개요: LLM을 활용한 자율운항선박 강화학습용 보상함수 설계 프로세스**

본 연구는 거대언어모델(LLM)을 활용하여 자율운항선박의 강화학습에 필요한 보상함수를 설계하는 프로세스를 제안하고 검증합니다. 기존의 보상함수 설계는 선박 운동의 비선형성으로 인해 많은 시행착오와 전문 지식이 필요했지만, 본 연구에서는 자연어 프롬프트와 사용자 피드백을 통해 보상함수를 자동으로 생성하고 점진적으로 개선하는 시스템을 구축했습니다.

* **프로젝트명:** 자율운항선박의 강화학습에 필요한 보상함수의 설계를 위한 거대언어모델(LLM) 적용 프로세스의 구성
* **발표 학회:** 2025년 한국항해항만학회 춘계학술대회 (2025, Spring Conference of KAOSTS)
* **연구진:**
    * 동서대학교: 김경현, 이병국
    * 부산대학교: 박세현, 정광효
    * 동의대학교: 이재용
* **연구 지원:** 본 연구는 과학기술정보통신부의 재원으로 한국연구재단의 지원을 받아 수행되었습니다 (No.RS-2023-00247537).

---

### **연구 배경 및 목표**

#### **문제점**

자율운항선박 제어에 강화학습을 적용할 때, 보상함수 설계는 매우 중요하지만 다음과 같은 어려움이 존재합니다.

* **복잡성:** 선박 운동의 높은 비선형성으로 인해 정교한 보상함수를 설계하기 어렵고, 조건 및 상태 변수가 증가할수록 복잡도와 소요 시간이 늘어납니다.
* **시행착오:** 보상 항목을 추가하거나 수치를 조정하는 데 수많은 시행착오가 필요합니다.
* **전문가 의존성:** 설계 과정이 전문가의 경험에 크게 의존하여 비전문가의 접근이 어렵습니다.

#### **연구 목표 및 방향**

본 연구는 NVIDIA의 EUREKA 방법론을 자율운항선박 환경에 맞게 적용하는 것을 목표로 합니다.

* LLM을 통해 복잡한 환경에서도 보상함수를 신속하게 생성합니다.
* 사용자 피드백을 활용하여 보상함수의 성능을 반복적으로 향상시킵니다.
* 자연어 기반의 설계를 통해 비전문가도 보상 함수를 생성할 수 있도록 지원합니다.

---

### **제안 방법론: LLM 기반 보상함수 설계 프로세스**

본 연구에서는 LLM을 통해 보상함수를 생성하고, 시뮬레이션 결과에 대한 사용자 피드백을 반영하여 함수를 개선하는 순환 구조의 프로세스를 제안합니다.


*<p align="center"><img src="/images/2025springconference/process.png" alt="process" width="900" />그림 1: 제안된 프로세스 개요</p>*


**프로세스 단계:**
1.  **프롬프트 작성:** 사용자는 시스템 환경 코드, 임무 목표, 이전 대화 기록 등을 자연어로 구성된 프롬프트로 작성합니다.
2.  **보상함수 생성:** 프롬프트는 OpenAI API (GPT-4o)를 통해 LLM에 전달되고, LLM은 파이썬 코드로 된 보상함수를 생성합니다.
3.  **강화학습 및 시뮬레이션:** 생성된 보상함수는 Unreal Engine 기반 시뮬레이터에 적용되어 DDPG 알고리즘으로 선박의 접안 학습을 진행합니다.
4.  **결과 평가 및 피드백:** 사용자는 시뮬레이션 결과를 분석하고, 개선이 필요한 부분을 자연어 피드백으로 작성합니다.
5.  **반복 개선:** 이 피드백을 기존 프롬프트에 추가하여 LLM에 다시 전달함으로써 보상함수를 점진적으로 최적화합니다.

---

### **시뮬레이션 환경 및 설정**

* **시뮬레이션 환경:** 부산항 신선대 컨테이너 터미널(북항) 주변 해역을 Unreal Engine 4.27.2로 구현
* **사용 모델 및 라이브러리:**
    * **LLM:** GPT-4o
    * **강화학습:** Stable Baselines (DDPG 알고리즘)
    * **선박 모델:** 40K급 벌크선 (Kose의 저속 조종 모델)
* **초기 및 목표 조건:**
    * **출발 지점:** 위도 35.089150°, 경도 129.102092°
    * **목표 지점:** 위도 35.105841°, 경도 129.093083°
    * **초기 속도/방향각:** 0 m/s, 305.67°
* **성공 조건:** 목표 지점에 도달하여 300초간 머무르는 것

---

### **실험 결과: 피드백을 통한 점진적 개선**

LLM에 초기 프롬프트를 입력하고, 시뮬레이션 결과를 바탕으로 피드백을 추가하며 보상함수를 3단계에 걸쳐 개선했습니다.

#### **1차 시도**
* **요구사항:** "목표 지점에 접근 시 긍정 보상을, 도달 시 큰 보상을 주며 완전히 정지해야 함"
* **결과:** 선박이 후진하거나, 목표에 가까워져도 음수 보상을 받는 등 문제가 발생
* **피드백:** "후진 대신 전진하도록 보상 체계 수정" 및 "목표 접근 시 양수 보상 명시"

#### **2차 시도**
* **결과:** 목표 방향으로 잘 기동하고 접안 준비 동작(감속)을 보였으나, 충돌을 회피하지 못하고 장애물을 통과함
* **피드백:** "장애물과 충돌 시 페널티 적용"

#### **3차 시도**
* **결과:** 충돌 페널티가 추가된 보상함수를 통해 선박이 장애물을 회피하며 성공적으로 목표에 도달하는 것을 확인함. 학습이 진행됨에 따라 보상 그래프가 점차 개선되는 양상을 보임.

*<p align="center"><img src="/images/2025springconference/Figure_1.png" alt="process" width="900" />그림 2: 학습 결과</p>*

---

### **결론 및 향후 연구**

본 연구를 통해 LLM과 사용자 피드백의 반복적인 상호작용만으로 자율운항선박 접안을 위한 강화학습 보상함수를 효과적으로 설계하고 개선할 수 있음을 입증했습니다. 이는 보상함수 설계의 자동화 및 협업적 설계의 가능성을 제시합니다.

향후에는 다음과 같은 연구를 통해 본 방법론을 더욱 발전시킬 계획입니다.

* 정량적 제약 조건(예: 속도 제한)을 더 정확하게 반영하기 위한 프롬프트 전략 고도화
* 다양한 항만 및 장거리 운항 시나리오에 대한 프레임워크 확장